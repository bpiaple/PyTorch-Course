{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06 Transfert Learning with PyTorch\n",
    "\n",
    "What is Transfer Learning?\n",
    "\n",
    "Transfer learning is a powerful technique in machine learning where knowledge gained from solving one problem is applied to a different but related problem.  Instead of starting the training process from scratch, you leverage a pre-trained model that has already learned useful features from a large dataset. This pre-trained model serves as a starting point, and you fine-tune it on your specific task, which typically requires less data and computational resources than training a model from scratch.\n",
    "\n",
    "Here's a breakdown of the key concepts:\n",
    "\n",
    "**1. Pre-trained Model:** This is a model that has been trained on a massive dataset, usually for a general task like image classification (e.g., ImageNet).  These models have learned a rich set of features that can be useful for other related tasks.\n",
    "\n",
    "**2. Feature Extraction:**  Pre-trained models are excellent feature extractors.  The early layers of the model learn general features (like edges, textures), while later layers learn more specific features (like shapes, objects).  You can use a pre-trained model to extract these features from your data without retraining the entire model.\n",
    "\n",
    "**3. Fine-tuning:**  This involves taking a pre-trained model and adapting it to your specific task. You typically replace the final layer(s) of the pre-trained model with layers suited for your task (e.g., a new classification layer with the correct number of classes). Then, you train the modified model on your dataset.  You can choose to freeze the weights of the early layers (to preserve the general features) and only train the later layers, or you can train the entire model with a lower learning rate.\n",
    "\n",
    "**4. When to Use Transfer Learning:**\n",
    "\n",
    "* **Limited Data:**  Transfer learning is especially beneficial when you have a small dataset for your target task.  Training a complex model from scratch on limited data often leads to overfitting.\n",
    "* **Computational Constraints:** Fine-tuning a pre-trained model is much faster and requires fewer resources than training from scratch.\n",
    "* **Improved Performance:**  Transfer learning can often lead to significantly better performance, especially when the source and target tasks are closely related.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Imagine you want to build an image classifier to identify different types of flowers. You have a small dataset of flower images. Instead of training a CNN from scratch, you can use a pre-trained model like ResNet, which has been trained on ImageNet (a massive dataset of general images). You remove ResNet's final classification layer (which classifies ImageNet categories) and replace it with a new classification layer with the number of flower types you want to classify. Then, you fine-tune the modified ResNet on your flower image dataset. The pre-trained features learned by ResNet on ImageNet will help the model learn to classify flowers more efficiently and accurately.\n",
    "\n",
    "**Key Benefits:**\n",
    "\n",
    "* Faster training\n",
    "* Reduced data requirements\n",
    "* Improved performance (often)\n",
    "* Easier prototyping\n",
    "\n",
    "Transfer learning is a widely used and highly effective technique in deep learning, allowing you to leverage the power of large pre-trained models to solve a wide range of problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu121\n",
      "0.19.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision.transforms import v2 as transforms\n",
    "\n",
    "# Try to get torchinfo, install it if it doen't work\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"torchinfo not found, installing it\")\n",
    "    %pip install torchinfo\n",
    "    from torchinfo import summary\n",
    "\n",
    "# Try to import the going_modular directory, if it doesn't work, clone the repo\n",
    "try:\n",
    "    from going_modular import data_setup, engine\n",
    "except:\n",
    "    print(\"going_modular not found, cloning the repo\")\n",
    "    !git clone https://gi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
